# -*- coding: utf-8 -*-
"""transforming_Target.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1grKe5I2KCoLJDIwWoZGLqn4dhRPqKm4a

<center>
    <img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/IDSNlogo.png" width="300" alt="cognitiveclass.ai logo">
</center>


# Machine Learning Foundation

## Section 2, Part a: Regression Intro: Transforming Target

## Learning objectives

By the end of this lesson, you will be able to:

* Apply transformations to make target variable more normally distributed for Regression
* Apply inverse transformations to be able to use these in a Regression context
"""

!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install scipy
!pip install scikit-learn

"""Explanation
def warn(*args, **kwargs): pass: This defines a function called warn that accepts any arguments (*args for positional arguments and **kwargs for keyword arguments) but does nothing (pass means “do nothing”).
warnings.warn = warn: This line assigns the custom warn function to warnings.warn. So, every time warnings.warn is called, it actually runs the empty warn function defined above, which does nothing.
Why Use This?
This code effectively suppresses all warnings in the script. By overriding warnings.warn, you prevent any warnings from being printed, which might be helpful when you want a clean output without warning messages (e.g., when you're testing or debugging and already aware of potential issues).

However, be careful with this, as it can hide useful warnings that might indicate important issues in your code!
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline


# Surpress warnings:
'''
def warnfun(*args, **kwargs):
    pass
import warnings
warnings.warn= warnfun
'''

"""In the following cells we will load the data and define some useful plotting functions.

"""

np.random.seed(72018)

def to_2d(array):
    return array.reshape(array.shape[0], -1)

def plot_exponential_data():
    data = np.exp(np.random.normal(size=1000))
    plt.hist(data)
    plt.show()
    return data

def plot_square_normal_data():
    data = np.square(np.random.normal(loc=5, size=1000))
    plt.hist(data)
    plt.show()
    return data

!wget "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/boston_housing_clean.pickle"

"""### Loading the Boston Housing Data

"""

with open('boston_housing_clean.pickle', 'rb') as to_read:
    boston = pd.read_pickle(to_read)
    #print(type(boston)) # will be return the class of boston.
    #print(len(boston)) #return the length of the dictionary.
    #print(boston.keys()) # method will return a list of all the keys in the dictionary.
    #print(boston.items()) # method will return a list of all the values in the dictionary.
    #print(boston.values()) # method will return each item in a dictionary, as tuples in a list.

boston_data = boston['dataframe'] #dataframe
boston_description = boston['description'] # string
print('Type boston_data is :',type(boston_data))
print('Type boston_description is :',type(boston_description))

description_boston={'CRIM':'per capita crime rate by town',
                    'ZN':'proportion of residential land zoned for lots over 25,000 sq.ft.',
                    'INDUS':'proportion of non-retail business acres per town',
                   'CHAS':'Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)',
                    'NOX':'nitric oxides concentration (parts per 10 million)',
                    'RM':'average number of rooms per dwelling',
                    'AGE':'proportion of owner-occupied units built prior to 1940',
                    'DIS':'weighted distances to five Boston employment centres',
                    'RAD':'index of accessibility to radial highways',
                    'TAX':'full-value property-tax rate per $10,000',
                    'PTRATIO':'pupil-teacher ratio by town',
                    'B':'1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town',
                    'LSTAT':'lower status of the population',
                    'MEDV':'Median value of owner-occupied homes in $1000s'}
print(type(description_boston))

len(description_boston)

boston_data.head(15)

"""**Determining** **Normality**

Making our target variable normally distributed often will lead to better results

If our target is not normally distributed, we can apply a transformation to it and then fit our regression to predict the transformed values.

How can we tell if our target is normally distributed? There are two ways:

* Using a Visual approach
* Using a Statistical Test

#### Using a Visual approach

#### Plotting a histogram:
"""

boston_data.MEDV.hist();

"""```
# This is formatted as code
```

The histogram does not look normal due to its right tail.

#### Using a Statistical Test

Without getting into Bayesian vs. frequentist debates, for the purposes of this lesson, the following will suffice:

* This is a statistical test that tests whether a distribution is normally distributed or not. It isn't perfect, but suffice it to say:
    * This test outputs a **p-value**. The _higher_ this p-value is the _closer_ the distribution is to normal.
    * Frequentist statisticians would say that you accept that the distribution is normal (more specifically: fail to reject the null hypothesis that it is normal) if p > 0.05.
"""

from scipy.stats.mstats import normaltest # D'Agostino K^2 Test

normaltest(boston_data.MEDV.values) # pvalue=1.7583188871696095e-20 that meaning =0.000000000000000000017583188871696095

"""# p-value is _extremely_ low. Our **y** variable which we have been dealing with this whole time was not normally distributed!

### Apply transformations to make target variable more normally distributed for Regression

Linear Regression assumes a normally distributed residuals which can be aided by transforming **y** variable which is the target variable. Let's try some common transformations to try and get **y** to be normally distributed:

* Log Transformation
* Square root Transformation
* Box cox Transformation

### Log Transformation

The log transformation can transform data that is significantly skewed right to be more normally distributed:
"""

data = plot_exponential_data()

plt.hist(np.log(data));

boston_data.MEDV.hist();

"""**Apply transformation to Boston Housing data:**

"""

log_medv = np.log(boston_data.MEDV)
log_medv.hist();

normaltest(log_medv)

"""**Conclusion**: The output is closer to normal distribution, but still not completely normal.

### Square root Transformation

The square root transformation is another transformation that can transform non-normally distributed data into normally distributed data:
"""

data = plot_square_normal_data()

"""You may notice that the output still exhibits a slight right skew.

"""

plt.hist(np.sqrt(data));

"""#### Exercise

Apply the square root transformation to the Boston Housing data target and test whether the result is normally distributed.
"""

boston_data.MEDV.hist();

## Enter your code here
sqrt_medv = np.sqrt(boston_data.MEDV)
plt.hist(sqrt_medv)

"""<details><summary>Click here for a sample python solution</summary>

```python
sqrt_medv = np.sqrt(boston_data.MEDV)
plt.hist(sqrt_medv)

```

"""

normaltest(sqrt_medv)

"""### Box cox Transformation

The box cox transformation is a parametrized transformation that tries to get distributions "as close to a normal distribution as possible".

It is defined as:

$$ \text{boxcox}(y_i) = \frac{y_i^{\lambda} - 1}{\lambda} $$

You can think of as a generalization of the square root function: the square root function uses the exponent of 0.5, but box cox lets its exponent vary so it can find the best one.
"""

from scipy.stats import boxcox

bc_result = boxcox(boston_data.MEDV)
boxcox_medv = bc_result[0]
lam = bc_result[1]

lam

boston_data['MEDV'].hist();

plt.hist(boxcox_medv);

normaltest(boxcox_medv)

"""We find that the box cox results in a graph which is significantly more normally distributed (according to p value) than the other two distributions.This can be even above 0.05.

Now that we have a normally distributed y-variable, let's test Regression using this transformed target variables.

### Testing regression:
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import (StandardScaler,
                                   PolynomialFeatures)

lr = LinearRegression()

"""**Define and load the predictor (X) and Target(y) variables**

"""

y_col = "MEDV"

X = boston_data.drop(y_col, axis=1) #Independent Variables
y = boston_data[y_col] #Dependent Variables
# here we drop the boston_data['MEDV'] to sperte data to two type: feter and leabel

X

len(y)

"""so I have 506 row and 14 column

**Create Polynomial Features**
"""

pf = PolynomialFeatures(degree=2, include_bias=False)
X_pf = pf.fit_transform(X)

#X_pf
type(X_pf) #numpy.ndarray
X_pf.shape
# where the first dimension has 506 elements and the second has 104.

"""**Split the data into Training and Test Sets**   

The split ratio here is 0.7 and 0.3 which means we will assign **70%** data for training and **30%** data for testing

"""

X_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3,
                                                    random_state=72018)

X_train

X_test

"""**Normalize the training data using `StandardScaler` on `X_train`. Use fit_transform() function**

"""

s = StandardScaler()
X_train_s = s.fit_transform(X_train)

"""**Discuss: what transformation do we need to apply next?**

Apply the appropriate transformation.

"""

# Enter your code here
bc_result2 = boxcox(y_train)
y_train_bc = bc_result2[0]
lam2 = bc_result2[1]

"""<details><summary>Click here for a sample python solution</summary>

```python
    
bc_result2 = boxcox(y_train)
y_train_bc = bc_result2[0]
lam2 = bc_result2[1]

```

As before, we'll now:

1. Fit regression
1. Transform testing data
1. Predict on testing data
"""

y_train_bc.shape

lr.fit(X_train_s, y_train_bc)
X_test_s = s.transform(X_test)
y_pred_bc = lr.predict(X_test_s)

y_test

y_pred_bc

"""### Discussion

* Are we done?
* What did we predict?
* How would you interpret these predictions?

#### Apply inverse transformations to be able to use these in a Regression context

Every transformation has an inverse transformation. The inverse transformation of $f(x) = \sqrt{x}$ is $f^{-1}(x) = x^2$, for example. Box cox has an inverse transformation as well: notice that we have to pass in the lambda value that we found from before:
"""

from scipy.special import inv_boxcox

inv_boxcox(boxcox_medv, lam)[:10]

boston_data['MEDV'].values[:10]

"""Exactly the same, as we would hope!

### Exercise:

1. Apply the appropriate inverse transformation to `y_pred_bc`.
2. Calculate the $R^2$ using the result of this inverse transformation and `y_test`.  

**Hint:** Use the **inv_boxcox()** function to get the transformed predicted values
"""

#Enter your code here
y_pred_tran = inv_boxcox(y_pred_bc,lam2)
r2_score(y_test,y_pred_tran)

"""<details><summary>Click here for a sample python solution</summary>

```python
y_pred_tran = inv_boxcox(y_pred_bc,lam2)
r2_score(y_test,y_pred_tran)

```

## Practice Exercise:

### Determine the R^2 of a LinearRegression without the box cox transformation.
"""

# Enter your code here
lr = LinearRegression()
lr.fit(X_train_s,y_train)
lr_pred = lr.predict(X_test_s)
r2_score(y_test,lr_pred)